{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick tour of Jupyter notebooks\n",
    "\n",
    "For those who may be unfamiliar with Amazon SageMaker, Jupyter notebooks, and OpenAI Gym, this is a good starting place. First, we need to explain how to run cells. Try to run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi! This is a cell. Press the ▶ button above to run it\")\n",
    "print(\"You can also run a cell with Ctrl+Enter or Shift+Enter. Experiment a bit with that.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some important components of a Jupyter notebook: \n",
    "\n",
    "**Dashboard** - When the notebook server is first started, a browser will be opened to the notebook dashboard. The dashboard serves as a home page for the notebook. Its main purpose is to display the portion of the filesystem accessible by the user, and to provide an overview of the running kernels, terminals, and parallel clusters.\n",
    "\n",
    "**Files** - The files tab provides an interactive view of the portion of the filesystem which is accessible by the user. This is typically rooted by the directory in which the notebook server was started.\n",
    "\n",
    "**Notebook body** - The body of a notebook is composed of cells. Each cell contains either markdown, code input, code output, or raw text. Cells can be included in any order and edited at-will, allowing for a large ammount of flexibility for constructing a narrative. There are different types of cells:\n",
    "\n",
    "* **Markdown cells** - These are used to build a nicely formatted narrative around the code in the document. The majority of this lesson is composed of markdown cells.\n",
    "\n",
    "* **Code cells** - These are used to define the computational code in the document. They come in two forms: the *input cell* where the user types the code to be executed, and the *output cell* which is the representation of the executed code. Depending on the code, this representation may be a simple scalar value, or something more complex like a plot or an interactive widget.\n",
    "\n",
    "* **Raw cells** - These are used when text needs to be included in raw form, without execution or transformation.\n",
    "\n",
    "**Terminal** - The notebook application is able to spawn interactive terminal instances. A new terminal can be spawned from the dashboard by clicking on the **`Files`** tab, followed by the **`New`** dropdown button, and then selecting **`Terminal`**.\n",
    "\n",
    "**Shortcuts** - The following shortcuts have been found to be the most useful in day-to-day tasks:\n",
    "\n",
    " * Basic navigation: **`enter`**, **`shift-enter`**, **`up/k`**, **`down/j`**\n",
    " * Saving the notebook: **`s`**\n",
    " * Cell types: **`y`**, **`m`**, **`1-6`**, **`r`**\n",
    " * Cell creation: **`a`**, **`b`**\n",
    " * Cell editing: **`x`**, **`c`**, **`v`**, **`d`**, **`z`**, **`ctrl+shift+-`**\n",
    " * Kernel operations: **`i`**, **`.`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do a fresh install of Gym with Box2D\n",
    "\n",
    "Now that you've got the basics down, the rest of this notebook will explain the Lunar Lander environment, including Box2D and OpenAI Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yes y | pip uninstall gym; exit 0\n",
    "!pip install box2d-py\n",
    "!pip install gym[box2d]\n",
    "!pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this is the same path as the one shown above next to \"Location: ... Otherwise, change it\"\n",
    "Location = '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing the render function of the original lunar lander code by overwriting the class file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo cp ./src/lunar_lander-local.py {Location}/gym/envs/box2d/lunar_lander.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize {Location}/gym/envs/box2d/lunar_lander.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and make the lunar lander environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore some properties of this environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the states of this agent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "pos = self.lander.position\n",
    "vel = self.lander.linearVelocity\n",
    "\n",
    "state = [\n",
    "            (pos.x - VIEWPORT_W/SCALE/2) / (VIEWPORT_W/SCALE/2),\n",
    "            (pos.y - (self.helipad_y+LEG_DOWN/SCALE)) / (VIEWPORT_H/SCALE/2),\n",
    "            vel.x*(VIEWPORT_W/SCALE/2)/FPS,\n",
    "            vel.y*(VIEWPORT_H/SCALE/2)/FPS,\n",
    "            self.lander.angle,\n",
    "            20.0*self.lander.angularVelocity/FPS,\n",
    "            1.0 if self.legs[0].ground_contact else 0.0,\n",
    "            1.0 if self.legs[1].ground_contact else 0.0\n",
    "       ]\n",
    "```       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What states of the environment can be observed outside?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ That means that all 8 states are observable. In many cases, this is not true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What actions can the agent take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. No action, fire left engine, main engine, right engine.\n",
    "\n",
    "According to Pontryagin's maximum principle it's optimal to fire engine full throttle or turn it off. That's the reason this environment is OK to have discreet actions (engine on or off)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about rewards? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "        reward = 0\n",
    "        shaping = \\\n",
    "            - 100*np.sqrt(state[0]*state[0] + state[1]*state[1]) \\\n",
    "            - 100*np.sqrt(state[2]*state[2] + state[3]*state[3]) \\\n",
    "            - 100*abs(state[4]) + 10*state[6] + 10*state[7]   # And ten points for legs contact, the idea is if you\n",
    "                                                              # lose contact again after landing, you get negative reward\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        reward -= m_power*0.30  # less fuel spent is better, about -30 for heurisic landing\n",
    "        reward -= s_power*0.03\n",
    "\n",
    "        done = False\n",
    "        if self.game_over or abs(state[0]) >= 1.0:\n",
    "            done   = True\n",
    "            reward = -100\n",
    "        if not self.lander.awake:\n",
    "            done   = True\n",
    "            reward = +100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector.\n",
    "Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
    "If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or\n",
    "comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main\n",
    "engine is -0.3 points each frame. Firing side engine is -0.03 points each frame. Solved is 200 points.\n",
    "\n",
    "Landing outside landing pad is possible. Fuel is infinite, but we can account for usage, so an agent can learn to fly and then land on its first attempt. Please see source code for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step through the environment with a random agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will:\n",
    "\n",
    "1. Initialize the scene with ```env.reset()```\n",
    "1. Get a random action from the set of allowable actions for this agent in this environment with ```action = env.action_space.sample()```\n",
    "1. Perform a step using this random action ```env.step(action)```\n",
    "1. Repeat for 200 time steps\n",
    "\n",
    "### **IMPORTANT: Stop the animation by hitting the big blue power button before proceeding.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation\n",
    "\n",
    "f = plt.figure()\n",
    "ax = f.gca()\n",
    "\n",
    "state = env.reset()\n",
    "im = env.render()\n",
    "image = plt.imshow(im, interpolation='None', animated=True)\n",
    "\n",
    "#Do an experiment and collect images, then animate\n",
    "images = []\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    images.append(env.render())\n",
    "    \n",
    "def animate(frame_index, images):\n",
    "    image.set_data(images[frame_index])\n",
    "    ax.set_title(str(frame_index))\n",
    "    return image\n",
    "\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, interval=100, frames=200, fargs=(images,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard landing? Random actions make no sense! (kinda obvious) \n",
    "\n",
    "Can we try using some rules or heuristics to help land safely? When coding your AI in games, it is typical to construct rules or decision trees to provide the most sensible action from the current state that the agent is in right now. Perhaps control theory can help?\n",
    "\n",
    "_Note_: This low resolution world is what the agent sees, but we will be modifying it and rendering a better version of the world in the lab you will do next!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------\n",
    "\n",
    "The ```heuristic()``` function below uses the following logic:\n",
    "1. Point towards where the agent needs to land\n",
    "1. Limit rotation or provide a target for angle ```angle_targ```\n",
    "1. Hover above the center for soft, centered landing ```hover_targ```\n",
    "1. Use two PID controllers:\n",
    "    a. one for converging towards a target angle\n",
    "    b. another one for converging target vertical velocity for soft landings\n",
    "1. Output an appropriate action ``a=1,2,3 or 0``` to achieve the above targets\n",
    "\n",
    "\n",
    "### Wait, what is a PID controller?\n",
    "\n",
    "The basic idea behind a PID controller is to read a sensor, then compute the desired actuator output by calculating proportional, integral, and derivative responses and summing those three components to compute the output. For closed systems, a sensor is used to measure the process variable (here, angle and velocity) and provide feedback to the control system. The set point (or \"target\") is the desired or command value for the process variable, such as 100 degrees Celsius in the case of a temperature control system (here, these targets are for angle and velocity). At any given moment, the difference between the process variable and the set point is used by the control system algorithm (compensator), to determine the desired actuator output to drive the system (here, the actuators are the engines and the engines, one main and two side, are controlled by the actions you input into the environment). \n",
    "\n",
    "<img src=\"http://blog.opticontrols.com/wp-content/uploads/2011/03/PID-Controller.png\" width =\"50%\">\n",
    "\n",
    "### Ok, but how do you determine this? What are those magic coefficients in the PID equations below?\n",
    "\n",
    "```angle_todo = (angle_targ - s[4])*0.5 - (s[5])*1.0```\n",
    "\n",
    "```hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5```\n",
    "\n",
    "The process of setting the optimal gains for P, I and D to get an ideal response from a control system is called tuning. There are different methods of tuning of which the “guess and check” method and the Ziegler Nichols methods are popular. Assume that the tuning was done with trial and error and you know these coefficients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, back to constructing the heuristic function ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def heuristic(env, s):\n",
    "    # Heuristic for:\n",
    "    # 1. Testing. \n",
    "    # 2. Demonstration rollout.\n",
    "    angle_targ = s[0]*0.5 + s[2]*1.0         # angle should point towards center (s[0] is horizontal coordinate, s[2] hor speed)\n",
    "    if angle_targ >  0.4: angle_targ =  0.4  # more than 0.4 radians (22 degrees) is bad\n",
    "    if angle_targ < -0.4: angle_targ = -0.4\n",
    "    hover_targ = 0.55*np.abs(s[0])           # target y should be proporional to horizontal offset\n",
    "\n",
    "    # PID controller: s[4] angle, s[5] angularSpeed\n",
    "    angle_todo = (angle_targ - s[4])*0.5 - (s[5])*1.0\n",
    "    #print(\"angle_targ=%0.2f, angle_todo=%0.2f\" % (angle_targ, angle_todo))\n",
    "\n",
    "    # PID controller: s[1] vertical coordinate s[3] vertical speed\n",
    "    hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5\n",
    "    #print(\"hover_targ=%0.2f, hover_todo=%0.2f\" % (hover_targ, hover_todo))\n",
    "\n",
    "    if s[6] or s[7]: # legs have contact\n",
    "        angle_todo = 0\n",
    "        hover_todo = -(s[3])*0.5  # override to reduce fall speed, that's all we need after contact\n",
    "\n",
    "    if env.continuous:\n",
    "        a = np.array( [hover_todo*20 - 1, -angle_todo*20] )\n",
    "        a = np.clip(a, -1, +1)\n",
    "    else:\n",
    "        a = 0\n",
    "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05: a = 2\n",
    "        elif angle_todo < -0.05: a = 3\n",
    "        elif angle_todo > +0.05: a = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step through the environment with a heuristic agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only change here is that we use the above function suggest a \"good action\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation\n",
    "\n",
    "f2 = plt.figure()\n",
    "ax2 = f2.gca()\n",
    "\n",
    "state = env.reset()\n",
    "im = env.render()\n",
    "image = plt.imshow(im, interpolation='None', animated=True)\n",
    "\n",
    "#Do an experiment and collect images, then animate\n",
    "images = []\n",
    "for _ in range(200):\n",
    "    action = heuristic(env, state) # THIS LINE CHANGED !!!\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    images.append(env.render())\n",
    "    \n",
    "def animate(frame_index, images):\n",
    "    image.set_data(images[frame_index])\n",
    "    ax2.set_title(str(frame_index))\n",
    "    return image\n",
    "\n",
    "ani2 = matplotlib.animation.FuncAnimation(f2, animate, interval=100, frames=200, fargs=(images,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look a little better! \n",
    "**\"Why do I need Reinforcement Learning (RL) then?\"** ... For a simple environment and a simple agent with 4 actions, hand coded rules are manageable. In situations where hand coded rules are difficult to write, or impossible, RL comes to the rescue. \n",
    "\n",
    "This line in the above code is a \"policy\", specifically, a policy that we hand coded. This answers the question, _what is the best action I can take from the current state?_ \n",
    "\n",
    "```\n",
    "action = heuristic(env, state)\n",
    "```\n",
    "\n",
    "In general, it is difficult to write and hand code policies for agents, so algorithms in RL help **generate** an optimal policy for an agent in an environment such as this Lunar Lander one. Today we will be using RL to teach the agent how to land. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's go!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
